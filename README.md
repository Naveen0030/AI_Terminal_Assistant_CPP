# ğŸ§  Terminal AI Assistant (Offline GPT with Ollama)

A lightweight C++ terminal-based AI assistant using local LLMs via the [Ollama](https://ollama.com) API.

---

### ğŸ”§ Features

- ğŸ’» Terminal-based chat UI
- ğŸ¤– Local inference with Ollama models (no internet needed after setup)
- ğŸ“¦ Uses only:
  - `libcurl` (for HTTP requests)
  - `nlohmann/json` (for JSON parsing)
- ğŸ¨ ANSI-colored terminal output
- âœ… Fully works on **Windows**, without needing `vcpkg` or Visual Studio

---

### ğŸš€ Setup Guide (Windows)

#### 1. âœ… Install [Ollama](https://ollama.com/download)
After installation, run it once:
```bash
ollama run llama3
```
This downloads and runs the model locally via http://localhost:11434.

â„¹ï¸ Ollama must be running in the background before starting the assistant.
